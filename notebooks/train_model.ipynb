{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/Mgalvaz/license_plate-recognizer/blob/main/notebooks/train_model.ipynb",
      "authorship_tag": "ABX9TyMxM391iTuKVWqfHD1o98yY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mgalvaz/license_plate-recognizer/blob/main/notebooks/train_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.functional import F\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.transforms import v2\n",
        "from PIL import Image, ImageDraw, ImageFont"
      ],
      "metadata": {
        "id": "2rz5HBqY9gIZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_plate_text() -> str:\n",
        "  nums = ''.join(random.choices('0123456789', k=4))\n",
        "\n",
        "  consonants = \"BCDFGHJKLMNPQRSTVWXYZ\"\n",
        "  vowels = \"AEIOU\"\n",
        "  alphabet = consonants + vowels\n",
        "  weights = [10] * len(consonants) + [1] * len(vowels)\n",
        "  letters = ''.join(random.choices(alphabet, weights=weights, k=3))\n",
        "\n",
        "  return f\"{nums}  {letters}\"\n",
        "\n",
        "def augment_image_v2(img: Image.Image) -> torch.Tensor:\n",
        "  tr = v2.Compose([\n",
        "    v2.PILToTensor(),\n",
        "    v2.ToDtype(torch.float, True),\n",
        "    #v2.RandomRotation(degrees=8, fill=0.392),\n",
        "    #v2.RandomPerspective(distortion_scale=0.25, p=1.0, fill=0.392),\n",
        "    #v2.GaussianNoise(sigma=0.05),\n",
        "    #v2.RandomApply([v2.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2))], p=0.7),\n",
        "  ])\n",
        "  return tr(img)\n",
        "\n",
        "class SyntheticPlateDataset(Dataset):\n",
        "  \"\"\"\n",
        "  Dataset that generates fake synthetic spanish license plates and augments them to simulate perspective.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_samples: int = 10000):\n",
        "    super().__init__()\n",
        "    self.num_samples = num_samples\n",
        "    self.font_main = ImageFont.truetype('/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf', 25)\n",
        "    self.font_small = ImageFont.truetype('/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf', 8)\n",
        "    self.transform = augment_image_v2\n",
        "    self.translator = dict((l, n) for n, l in enumerate('ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789', start=1))\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return self.num_samples\n",
        "\n",
        "  def __getitem__(self, idx: int) -> tuple:\n",
        "    plate_text = generate_plate_text()\n",
        "    plate = Image.new(\"L\", (150, 32), color=230)\n",
        "    draw = ImageDraw.Draw(plate)\n",
        "    draw.rectangle([0, 0, 20, 32], fill=55)\n",
        "    draw.text((8, 18), 'E', font=self.font_small, fill=230)\n",
        "    draw.text((22, 2), plate_text, font=self.font_main, fill=50)\n",
        "    plate = self.transform(plate).to(device)\n",
        "    label = torch.tensor([self.translator[l] for l in plate_text if l != ' '], dtype=torch.long).to(device)\n",
        "    return plate, label"
      ],
      "metadata": {
        "id": "9o_uXhwL9gHL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict((l, n) for n, l in enumerate('ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789', start=1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHqQC-sFtsrx",
        "outputId": "5432810d-a3ba-4a23-b29d-6a1efdc5c7bb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, 'W': 23, 'X': 24, 'Y': 25, 'Z': 26, '0': 27, '1': 28, '2': 29, '3': 30, '4': 31, '5': 32, '6': 33, '7': 34, '8': 35, '9': 36}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch: list) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "  imgs, labels = zip(*batch)\n",
        "  imgs = torch.stack(imgs)\n",
        "  labels = pad_sequence(labels, batch_first=True, padding_value=-1)  # rellena con -1\n",
        "  return imgs, labels\n",
        "\n",
        "def ctc_decode(pred_seq: torch.Tensor, blank: int=0) -> list[int]:\n",
        "  decoded = []\n",
        "  prev = None\n",
        "  for p in pred_seq:\n",
        "    if p != blank and p != prev:\n",
        "      decoded.append(p)\n",
        "    prev = p\n",
        "  return decoded\n",
        "\n",
        "class CRNN(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CRNN, self).__init__()\n",
        "\n",
        "    self.cnn = nn.Sequential(\n",
        "      nn.Conv2d(1, 64, (3, 3), padding=1),  # (1, 32, 150) -> (32, 32, 150)\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(2, 2), # (32, 32, 150) -> (32, 16, 75)\n",
        "      nn.Conv2d(64, 128, (3, 3), padding=1),  # (32, 16, 75) -> (64, 16, 75)\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(2, 2),  # (64, 16, 75) -> (64, 8, 37)\n",
        "      nn.Conv2d(128, 256, (3, 3), padding=1),  # (64, 8, 37) -> (128, 8, 37)\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d((1, 2), (2,1)),  # (128, 8, 37) -> (128, 4, 36)\n",
        "      nn.Conv2d(256, 512, (3, 3), padding=1),  # (128, 4, 36) -> (256, 4, 36)\n",
        "      nn.BatchNorm2d(512),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(512, 512, (3, 3), padding=1),  # (256, 4, 37) -> (256, 4, 36)\n",
        "      nn.BatchNorm2d(512),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout2d(0.5),\n",
        "      nn.MaxPool2d((2, 1), 1),  # (256, 4, 36) -> (256, 3, 36)\n",
        "      nn.Conv2d(512, 512, (3, 3), padding=0),  # (256, 3, 36) -> (512, 1, 34)\n",
        "      nn.BatchNorm2d(512),\n",
        "      nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    self.rnn = nn.GRU(512, 256, num_layers=2, batch_first=True, bidirectional=True) # (36, 512) -> (512, 36)\n",
        "\n",
        "    self.fc = nn.Linear(512, 37) # (512, 36) -> (37, 36)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    _, ic, ih, iw = x.size() # (batch, channels=1, height=32, width=150)\n",
        "    assert (ic, ih, iw) == (1, 32, 150), f'Input size ({ic}, {ih}, {iw}) does not correspond to expected size (1, 32, 150)'\n",
        "    x = self.cnn(x) # (batch, channels=512, height=1, width=34)\n",
        "\n",
        "    x = x.squeeze(2).permute(0, 2, 1)  # (batch, width=34, channels=512)\n",
        "\n",
        "    x, _ = self.rnn(x) # (batch, seq_len=34, channels=512)\n",
        "    x = self.fc(x) # (batch, seq_len=34, label=37)\n",
        "    return x"
      ],
      "metadata": {
        "id": "_1QnvR-ONUDg"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2FMNqaNRmxg",
        "outputId": "d386008c-2ffa-4a64-8e8c-f5e99460d7ca"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRANSLATOR = dict((l, n) for n, l in enumerate('ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789', start=1))\n",
        "total = 10000\n",
        "num_test = total//7\n",
        "num_train = total - num_test\n",
        "num_epochs = 3\n",
        "dataset = SyntheticPlateDataset(num_samples=total)\n",
        "train_dataset, test_dataset = random_split(dataset, [num_train, num_test])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64,  collate_fn=collate_fn)\n",
        "\n",
        "#Model loading\n",
        "model = CRNN().to(device)\n",
        "criterion = nn.CTCLoss(zero_infinity=True)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "#Training\n",
        "for epoch in range(1, num_epochs+1):\n",
        "  loop = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")\n",
        "  for batch_images, batch_labels in loop:\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # forward\n",
        "    batch_output = model(batch_images)\n",
        "    log_probs = F.log_softmax(batch_output, dim=2).permute(1, 0 ,2)\n",
        "    input_lengths = torch.full(size=(batch_output.size(0),), fill_value=batch_output.size(1), dtype=torch.long)\n",
        "    targets = batch_labels\n",
        "    batch_labels = [lbl[lbl != -1] for lbl in batch_labels]\n",
        "    target_lengths = torch.tensor([len(l) for l in batch_labels], dtype=torch.long)\n",
        "    loss = criterion(log_probs, targets, input_lengths, target_lengths)\n",
        "    # backward\n",
        "    loss.backward()\n",
        "    # optimize\n",
        "    optimizer.step()\n",
        "    loop.set_postfix(loss=loss.item())\n",
        "\n",
        "\n",
        "#Testing\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "  for batch_images, batch_labels in test_loader:\n",
        "    output = model(batch_images).permute(1, 0 ,2)\n",
        "    preds = output.argmax(dim=2).cpu().numpy().T\n",
        "\n",
        "    batch_labels = [lbl[lbl != -1] for lbl in batch_labels]\n",
        "    decoded_preds = [ctc_decode(seq) for seq in preds]\n",
        "\n",
        "    for pred, target in zip(decoded_preds, batch_labels):\n",
        "      target = target.cpu().numpy().tolist()\n",
        "      if pred == target:\n",
        "        correct += 1\n",
        "  print(f\"Exact match accuracy: {correct / num_test:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131,
          "referenced_widgets": [
            "4733c15d1fc44de29f426c7b26001950",
            "46c69f5f2f6e420883830e069ba2797e",
            "dac8a60e2e554549953b4a0b6bb40ccb",
            "e7420dc8a36d4a0ca6b5acd396671048",
            "273330d6b3604437aad827279f5fbad8",
            "7fa1b2b22e8e4a5d81f3082b8c3cc540",
            "8908e9550aad4a91a9c3807883706616",
            "6a94cfb14ab2466d8899ee3f180f4920",
            "d63923b750024a2a9bff354617ec4268",
            "bfc247f0b7de439a8ae120dc29ba4413",
            "0e984a21b1214ff9bb25d1d94916cf66",
            "b6a606882ff24388af8460d60f2d403e",
            "6d08fe5e8f5a40f59728b2b2645394a5",
            "880060bc2d724daa8e70ec5fea512880",
            "df9a1f6ceaed4e9f9169ff4c491f6759",
            "521d48e168c14eb7a2912c8df9f0081f",
            "50284bac0a424e4f8249b1c725b47439",
            "cf44c4949b6b4620846639800e80f8ee",
            "79702e312670442786057011290522f9",
            "2fd75fff8a0945d6b5ad7a5d409b323d",
            "34c8ed24b49b44c6b9d8cb2e742cd3cd",
            "7877a942897f47d69389f5dd734362a0",
            "a0bed28d2d83408ab67e51ba8a87e150",
            "4f51bb6ba5104846ac2bee8b1c861a06",
            "26e4658a0e064471bd34377d3dab00cc",
            "8092610e86aa4377902df77133efe2a2",
            "afca7a1943bb4153b109f191bc5abb88",
            "e68cb7557977499192678abffbf3a698",
            "09abd502c6b540059f7d338acde5b5f7",
            "ccb97ed623334bad901cc9b1bd7dc547",
            "733f10867d88405bbbc528e9c92bd534",
            "b660d99f07d341478747941607e33816",
            "4517ab5415b94d99ba88e1d5cfbcd704"
          ]
        },
        "id": "6YzOS4TgNv4Q",
        "outputId": "b408bc24-ff2e-498f-d02c-d0e4593df405"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/3:   0%|          | 0/134 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4733c15d1fc44de29f426c7b26001950"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 2/3:   0%|          | 0/134 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6a606882ff24388af8460d60f2d403e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 3/3:   0%|          | 0/134 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0bed28d2d83408ab67e51ba8a87e150"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact match accuracy: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REVERSE_DICT = {v: k for k, v in TRANSLATOR.items()}\n",
        "final_dataset = SyntheticPlateDataset(num_samples=2)\n",
        "final_loader = DataLoader(final_dataset, batch_size=64,  collate_fn=collate_fn)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for batch_images, batch_labels in final_loader:\n",
        "    output = model(batch_images)\n",
        "    preds = output.argmax(dim=2).cpu().numpy().T\n",
        "    print(output)\n",
        "\n",
        "    batch_labels = [lbl[lbl != -1] for lbl in batch_labels]\n",
        "    decoded_preds = [ctc_decode(seq) for seq in preds]\n",
        "\n",
        "    for pred, target in zip(decoded_preds, batch_labels):\n",
        "      target = target.cpu().numpy().tolist()\n",
        "      print(pred, target, list(map(lambda x: REVERSE_DICT[x], target)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYzQdWVsVzHy",
        "outputId": "245aaab4-d70c-4860-af7c-64eb1188f2de"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 6.2799, -3.9095, -3.3256,  ...,  0.9906,  1.7291,  1.3155],\n",
            "         [ 6.1752, -3.8151, -3.2253,  ...,  0.8959,  1.5170,  1.2116],\n",
            "         [ 6.1866, -3.7950, -3.2009,  ...,  0.8611,  1.4974,  1.1791],\n",
            "         ...,\n",
            "         [ 3.3325,  1.6309,  3.6533,  ..., -2.3319, -0.8925, -1.6921],\n",
            "         [ 3.3319,  1.6384,  3.6663,  ..., -2.3395, -0.8902, -1.6960],\n",
            "         [ 3.3194,  1.6501,  3.6797,  ..., -2.3490, -0.8964, -1.7029]],\n",
            "\n",
            "        [[ 6.8638, -4.4178, -3.5406,  ...,  3.0256,  3.0736,  3.3268],\n",
            "         [ 6.7867, -4.3937, -3.5195,  ...,  2.9951,  3.0113,  3.3548],\n",
            "         [ 6.7900, -4.3820, -3.4978,  ...,  2.9829,  3.0075,  3.3574],\n",
            "         ...,\n",
            "         [ 3.3261,  1.6094,  3.6365,  ..., -2.3124, -0.8833, -1.6755],\n",
            "         [ 3.3264,  1.6000,  3.6309,  ..., -2.3083, -0.8751, -1.6716],\n",
            "         [ 3.3141,  1.6174,  3.6480,  ..., -2.3220, -0.8845, -1.6823]]],\n",
            "       device='cuda:0')\n",
            "[] [29, 33, 28, 29, 13, 20, 7] ['2', '6', '1', '2', 'M', 'T', 'G']\n",
            "[] [34, 32, 27, 30, 19, 16, 22] ['7', '5', '0', '3', 'S', 'P', 'V']\n"
          ]
        }
      ]
    }
  ]
}