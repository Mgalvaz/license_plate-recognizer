{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/Mgalvaz/license_plate-recognizer/blob/main/notebooks/train_model.ipynb",
      "authorship_tag": "ABX9TyMxNpqtvNIuWxavZoIiySNm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mgalvaz/license_plate-recognizer/blob/main/notebooks/train_OCRmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.functional import F\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.transforms import v2\n",
        "from PIL import Image, ImageDraw, ImageFont"
      ],
      "metadata": {
        "id": "2rz5HBqY9gIZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_plate_text() -> str:\n",
        "  nums = ''.join(random.choices('0123456789', k=4))\n",
        "\n",
        "  consonants = \"BCDFGHJKLMNPQRSTVWXYZ\"\n",
        "  vowels = \"AEIOU\"\n",
        "  alphabet = consonants + vowels\n",
        "  weights = [10] * len(consonants) + [1] * len(vowels)\n",
        "  letters = ''.join(random.choices(alphabet, weights=weights, k=3))\n",
        "\n",
        "  return f\"{nums} {letters}\"\n",
        "\n",
        "def augment_image_v2(img: Image.Image) -> torch.Tensor:\n",
        "  tr = v2.Compose([\n",
        "    v2.PILToTensor(),\n",
        "    v2.ToDtype(torch.float, True),\n",
        "    v2.RandomRotation(degrees=8, fill=0.392),\n",
        "    v2.RandomPerspective(distortion_scale=0.25, p=1.0, fill=0.392),\n",
        "    v2.GaussianNoise(sigma=0.05),\n",
        "    v2.RandomApply([v2.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2))], p=0.7),\n",
        "  ])\n",
        "  return tr(img)\n",
        "\n",
        "class SyntheticPlateDataset(Dataset):\n",
        "  \"\"\"\n",
        "  Dataset that generates fake synthetic spanish license plates and augments them to simulate perspective.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_samples: int = 10000):\n",
        "    super().__init__()\n",
        "    self.num_samples = num_samples\n",
        "    self.font_main = ImageFont.truetype('/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf', 27)\n",
        "    self.font_small = ImageFont.truetype('/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf', 6)\n",
        "    self.transform = augment_image_v2\n",
        "    self.translator = dict((l, n) for n, l in enumerate('ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789', start=1))\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return self.num_samples\n",
        "\n",
        "  def __getitem__(self, idx: int) -> tuple:\n",
        "    plate_text = generate_plate_text()\n",
        "    plate = Image.new(\"L\", (150, 32), color=230)\n",
        "    draw = ImageDraw.Draw(plate)\n",
        "    draw.rectangle([3, 3, 15, 29], fill=134)\n",
        "    draw.text((7, 19), 'E', font=self.font_small, fill=230)\n",
        "    draw.text((20, 2), plate_text, font=self.font_main, fill=50)\n",
        "    plate = self.transform(plate).to(device)\n",
        "    label = torch.tensor([self.translator[l] for l in plate_text if l != ' '], dtype=torch.long).to(device)\n",
        "    return plate, label"
      ],
      "metadata": {
        "id": "9o_uXhwL9gHL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRANSLATOR = dict((l, n) for n, l in enumerate('ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789', start=1))\n",
        "NUM_CLASSES = len(TRANSLATOR) + 1\n"
      ],
      "metadata": {
        "id": "PHqQC-sFtsrx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch: list) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "  imgs, labels = zip(*batch)\n",
        "  imgs = torch.stack(imgs)\n",
        "  labels = pad_sequence(labels, batch_first=True, padding_value=-1)\n",
        "  return imgs, labels\n",
        "\n",
        "def ctc_decode(pred_seq: torch.Tensor, blank: int=0) -> list[int]:\n",
        "  decoded = []\n",
        "  prev = None\n",
        "  for p in pred_seq:\n",
        "    if p != blank and p != prev:\n",
        "      decoded.append(p)\n",
        "    prev = p\n",
        "  return decoded\n",
        "\n",
        "class CRNN(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.cnn = nn.Sequential(\n",
        "      nn.Conv2d(1, 64, (3, 3), padding=1),  # (1, 32, 150) -> (32, 32, 150)\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(2, 2), # (32, 32, 150) -> (32, 16, 75)\n",
        "      nn.Conv2d(64, 128, (3, 3), padding=1),  # (32, 16, 75) -> (64, 16, 75)\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(2, 2),  # (64, 16, 75) -> (64, 8, 37)\n",
        "      nn.Conv2d(128, 256, (3, 3), padding=1),  # (64, 8, 37) -> (128, 8, 37)\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d((1, 2), (2,1)),  # (128, 8, 37) -> (128, 4, 36)\n",
        "      nn.Conv2d(256, 512, (3, 3), padding=1),  # (128, 4, 36) -> (256, 4, 36)\n",
        "      nn.BatchNorm2d(512),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(512, 512, (3, 3), padding=1),  # (256, 4, 37) -> (256, 4, 36)\n",
        "      nn.BatchNorm2d(512),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout2d(0.5),\n",
        "      nn.MaxPool2d((2, 1), 1),  # (256, 4, 36) -> (256, 3, 36)\n",
        "      nn.Conv2d(512, 512, (3, 3), padding=0),  # (256, 3, 36) -> (512, 1, 34)\n",
        "      nn.BatchNorm2d(512),\n",
        "      nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    self.rnn = nn.GRU(512, 256, num_layers=2, batch_first=True, bidirectional=True) # (34, 512) -> (512, 34)\n",
        "\n",
        "    self.decoder = nn.Linear(512, NUM_CLASSES) # (512, 34) -> (37, 34)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    _, ic, ih, iw = x.size() # (batch, channels=1, height=32, width=150)\n",
        "    assert (ic, ih, iw) == (1, 32, 150), f'Input size ({ic}, {ih}, {iw}) does not correspond to expected size (1, 32, 150)'\n",
        "    x = self.cnn(x) # (batch, channels=512, height=1, width=34)\n",
        "\n",
        "    x = x.squeeze(2).permute(0, 2, 1)  # (batch, width=34, channels=512)\n",
        "\n",
        "    x, _ = self.rnn(x) # (batch, seq_len=34, channels=512)\n",
        "\n",
        "    x = self.decoder(x) # (batch, seq_len=34, label=37)\n",
        "    return x"
      ],
      "metadata": {
        "id": "_1QnvR-ONUDg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "#Model loading\n",
        "model = CRNN().to(device)\n",
        "criterion = nn.CTCLoss(blank=0, zero_infinity=True, reduction='sum')\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
        "last_epoch = 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2FMNqaNRmxg",
        "outputId": "6dd3efef-dc96-4f5c-a728-9c12c7b44fd4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('OCR_model_3.pth', map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "last_epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "print(f'Checkpoint loaded. Last epoch: {last_epoch}, with loss: {loss}')\n"
      ],
      "metadata": {
        "id": "G3iUqBMCLsRu",
        "outputId": "9f7d5d90-d0af-43a5-9c48-a7bfb3305ab3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint loaded. Last epoch: 5, with loss: 6.454759120941162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total = 70000\n",
        "num_test = total//7\n",
        "num_train = total - num_test\n",
        "dataset = SyntheticPlateDataset(num_samples=total)\n",
        "train_dataset, test_dataset = random_split(dataset, [num_train, num_test])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64,  collate_fn=collate_fn)\n",
        "\n",
        "#Training\n",
        "model.train()\n",
        "num_epochs = 5 + last_epoch\n",
        "for epoch in range(last_epoch+1, num_epochs+1):\n",
        "  loop = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")\n",
        "  for batch_images, batch_labels in loop:\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # forward\n",
        "    batch_output = model(batch_images)\n",
        "    log_probs = F.log_softmax(batch_output, dim=2).permute(1, 0 ,2)\n",
        "    input_lengths = torch.full(size=(batch_output.size(0),), fill_value=batch_output.size(1), dtype=torch.long)\n",
        "    targets = batch_labels\n",
        "    batch_labels = [lbl[lbl != -1] for lbl in batch_labels]\n",
        "    target_lengths = torch.tensor([len(l) for l in batch_labels], dtype=torch.long)\n",
        "    loss = criterion(log_probs, targets, input_lengths, target_lengths)\n",
        "    # backward\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    # optimize\n",
        "    optimizer.step()\n",
        "    loop.set_postfix(loss=loss.item())\n",
        "\n",
        "\n",
        "#Testing\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "  for batch_images, batch_labels in test_loader:\n",
        "    output = model(batch_images).permute(1, 0 ,2)\n",
        "    preds = output.argmax(dim=2).cpu().numpy().T\n",
        "\n",
        "    batch_labels = [lbl[lbl != -1] for lbl in batch_labels]\n",
        "    decoded_preds = [ctc_decode(seq) for seq in preds]\n",
        "\n",
        "    for pred, target in zip(decoded_preds, batch_labels):\n",
        "      target = target.cpu().numpy().tolist()\n",
        "      if pred == target:\n",
        "        correct += 1\n",
        "  print(f\"Exact match accuracy: {correct / num_test:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195,
          "referenced_widgets": [
            "5161a7a103e342718d1858ca92ae4313",
            "018a9d5f7748435ba9f946e379fb10c9",
            "82ebd8db7832471ba7c1e4da3ae56262",
            "ad6be12ab30447a1af859de90bd15466",
            "550081b5497a424bbd6b3913fa3c1736",
            "bbda8afcb43b4ed8b930ad120592df23",
            "8f146b8f32474a82a67fde98dba90038",
            "4f57eab493b94c2bb0a23bed48e9dc79",
            "209c567f4d59463da9d44071079a9af8",
            "26ffd238b9f74130adbbeced5d9c6c58",
            "0f9a904c20874f5d846f65cd1cac9e32",
            "6ed0aab50b854986b22a8cc82517254a",
            "08c600a06f1e432095398d2b35344de3",
            "87a8ebafa2dd48699feb4a938741e0f9",
            "bb0b66c5ba794e36b7dc4659633c6e24",
            "4896e36232364a79a29288815644fe05",
            "0716e8dba4f249aaa0f9159718a3bb1c",
            "1a4c6ee7227e4c09be9ec5bfc50b7fa4",
            "336f97fa94bf445184aecb879891a3bf",
            "34b012c998fa4dd7a220037b5eec2587",
            "b96c887fc5874b8c8e27d12c92d57d27",
            "54d0c793161b497ab6ad0953f6c3c01b",
            "0ecfc0ae0c4c418ca88311b62cf46e76",
            "8f5180b53ef64a01b98734b91db04903",
            "18b79baae27e477abf77d06dcfcfd041",
            "b08b2e3043d9484d8fd5225c2eebe668",
            "9671c0ae20db43ccb0b97e3f250b7fe2",
            "a0738f6f2cd14f5e85a42c9edaba0d3d",
            "ac96539dc79b4bd19851331fdd57036e",
            "9889fe9f46b3488d8f458c8c3423168b",
            "f88ca8c7bba942cba64477372ce85bff",
            "9c05b9b0a7464900b42aafc272eabb3d",
            "c699c94e31e34470a820d1bf9536bd5a",
            "44a92d2fe29d4a8e8c6dec8b936c3881",
            "c0641527ceba44889c44861a66dc4824",
            "d335e1ecea8546cc84c3350e325c814d",
            "00647ad505f041e2b861c0d462352a3e",
            "52ee4056306b4be59f50ceb9b306dda4",
            "95fae7a339e74135a9547043afd5b319",
            "9e78d4797f834948906af1ca17179326",
            "df86efc6928d4dc1bd8e67eac408c79b",
            "9adc21d62fc343fc92edb2c80652aad9",
            "d9cdf0c4e0ea489ab8f4ae4a319e0850",
            "fe2ce393cd434ddf872b16bb036cdb33",
            "97278191acc84295bd420a6e4bbb0b5a",
            "b8f503461ad744beabbe74d55fda5252",
            "d834eb9f2faa4e47bc878228fc5709a3",
            "b4452fe625aa4cdd84c14ff49e8f800c",
            "05ed650ba7994e6e81bdf80573291172",
            "a52778eefa3c4e7ea88a618dcf965f81",
            "0db1c2479bea4005a64721a6fbbc44b2",
            "b7c350a86fd0469e8bc3cc166ace0c5c",
            "e139739c7b2f47f3b97fbd0d8d034c21",
            "1a453ecec4cb42f6a77ad6be87d772a3",
            "cf35aff51ef449ff912a6dbe0a09d8d5"
          ]
        },
        "id": "6YzOS4TgNv4Q",
        "outputId": "4277f02a-f236-4199-a39f-1effe388487c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 6/10:   0%|          | 0/938 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5161a7a103e342718d1858ca92ae4313"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 7/10:   0%|          | 0/938 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ed0aab50b854986b22a8cc82517254a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 8/10:   0%|          | 0/938 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ecfc0ae0c4c418ca88311b62cf46e76"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 9/10:   0%|          | 0/938 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44a92d2fe29d4a8e8c6dec8b936c3881"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 10/10:   0%|          | 0/938 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97278191acc84295bd420a6e4bbb0b5a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact match accuracy: 98.87%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REVERSE_DICT = {v: k for k, v in TRANSLATOR.items()}\n",
        "final_dataset = SyntheticPlateDataset(num_samples=20)\n",
        "final_loader = DataLoader(final_dataset, batch_size=64,  collate_fn=collate_fn)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for batch_images, batch_labels in final_loader:\n",
        "    output = model(batch_images).permute(1, 0 ,2)\n",
        "    preds = output.argmax(dim=2).cpu().numpy().T\n",
        "\n",
        "    batch_labels = [lbl[lbl != -1] for lbl in batch_labels]\n",
        "    decoded_preds = [ctc_decode(seq) for seq in preds]\n",
        "\n",
        "    for pred, target in zip(decoded_preds, batch_labels):\n",
        "      target = target.cpu().numpy().tolist()\n",
        "      print(pred, target, list(map(lambda x: REVERSE_DICT[x], target)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYzQdWVsVzHy",
        "outputId": "2544fdfa-7908-412e-cdba-f5b01433f5fb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[np.int64(29), np.int64(31), np.int64(32), np.int64(32), np.int64(22), np.int64(24), np.int64(7)] [29, 31, 32, 32, 22, 24, 7] ['2', '4', '5', '5', 'V', 'X', 'G']\n",
            "[np.int64(27), np.int64(35), np.int64(29), np.int64(34), np.int64(25), np.int64(24), np.int64(4)] [27, 35, 29, 34, 25, 24, 4] ['0', '8', '2', '7', 'Y', 'X', 'D']\n",
            "[np.int64(32), np.int64(33), np.int64(27), np.int64(29), np.int64(6), np.int64(4), np.int64(8)] [32, 33, 27, 29, 6, 4, 8] ['5', '6', '0', '2', 'F', 'D', 'H']\n",
            "[np.int64(28), np.int64(30), np.int64(34), np.int64(27), np.int64(20), np.int64(17), np.int64(14)] [28, 30, 34, 27, 20, 17, 14] ['1', '3', '7', '0', 'T', 'Q', 'N']\n",
            "[np.int64(30), np.int64(35), np.int64(32), np.int64(34), np.int64(6), np.int64(12), np.int64(3)] [30, 35, 32, 34, 6, 12, 3] ['3', '8', '5', '7', 'F', 'L', 'C']\n",
            "[np.int64(28), np.int64(31), np.int64(36), np.int64(32), np.int64(12), np.int64(11), np.int64(3)] [28, 31, 36, 32, 12, 11, 3] ['1', '4', '9', '5', 'L', 'K', 'C']\n",
            "[np.int64(31), np.int64(29), np.int64(36), np.int64(33), np.int64(17), np.int64(26), np.int64(25)] [31, 29, 36, 33, 17, 26, 25] ['4', '2', '9', '6', 'Q', 'Z', 'Y']\n",
            "[np.int64(32), np.int64(28), np.int64(35), np.int64(28), np.int64(11), np.int64(19), np.int64(23)] [32, 28, 35, 28, 11, 19, 23] ['5', '1', '8', '1', 'K', 'S', 'W']\n",
            "[np.int64(35), np.int64(29), np.int64(36), np.int64(34), np.int64(11), np.int64(3), np.int64(18)] [35, 29, 36, 34, 11, 3, 18] ['8', '2', '9', '7', 'K', 'C', 'R']\n",
            "[np.int64(33), np.int64(30), np.int64(29), np.int64(28), np.int64(2), np.int64(10), np.int64(15)] [33, 30, 29, 28, 2, 10, 15] ['6', '3', '2', '1', 'B', 'J', 'O']\n",
            "[np.int64(32), np.int64(35), np.int64(33), np.int64(30), np.int64(16), np.int64(7), np.int64(6)] [32, 35, 33, 30, 16, 7, 6] ['5', '8', '6', '3', 'P', 'G', 'F']\n",
            "[np.int64(27), np.int64(30), np.int64(32), np.int64(27), np.int64(24), np.int64(19), np.int64(26)] [27, 30, 32, 27, 24, 19, 26] ['0', '3', '5', '0', 'X', 'S', 'Z']\n",
            "[np.int64(30), np.int64(34), np.int64(34), np.int64(34), np.int64(19), np.int64(22), np.int64(25)] [30, 34, 34, 34, 19, 22, 25] ['3', '7', '7', '7', 'S', 'V', 'Y']\n",
            "[np.int64(31), np.int64(32), np.int64(32), np.int64(31), np.int64(19), np.int64(16), np.int64(6)] [31, 32, 32, 31, 19, 16, 6] ['4', '5', '5', '4', 'S', 'P', 'F']\n",
            "[np.int64(27), np.int64(33), np.int64(27), np.int64(31), np.int64(10), np.int64(8), np.int64(17)] [27, 33, 27, 31, 10, 8, 17] ['0', '6', '0', '4', 'J', 'H', 'Q']\n",
            "[np.int64(33), np.int64(35), np.int64(30), np.int64(32), np.int64(19), np.int64(26), np.int64(2)] [33, 35, 30, 32, 19, 26, 2] ['6', '8', '3', '5', 'S', 'Z', 'B']\n",
            "[np.int64(31), np.int64(32), np.int64(32), np.int64(34), np.int64(22), np.int64(2), np.int64(11)] [31, 32, 32, 34, 22, 2, 11] ['4', '5', '5', '7', 'V', 'B', 'K']\n",
            "[np.int64(32), np.int64(32), np.int64(34), np.int64(28), np.int64(14), np.int64(20), np.int64(24)] [32, 32, 34, 28, 14, 20, 24] ['5', '5', '7', '1', 'N', 'T', 'X']\n",
            "[np.int64(36), np.int64(33), np.int64(29), np.int64(28), np.int64(25), np.int64(22), np.int64(20)] [36, 33, 29, 28, 25, 22, 20] ['9', '6', '2', '1', 'Y', 'V', 'T']\n",
            "[np.int64(29), np.int64(36), np.int64(29), np.int64(33), np.int64(24), np.int64(11), np.int64(7)] [29, 36, 29, 33, 24, 11, 7] ['2', '9', '2', '6', 'X', 'K', 'G']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss.item(),\n",
        "            }, 'OCR_model_3.pth')"
      ],
      "metadata": {
        "id": "msf5tPSSHm5G"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}